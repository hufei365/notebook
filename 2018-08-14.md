[原文地址：Backpressuring in Streams](https://nodejs.org/en/docs/guides/backpressuring-in-streams/)

## Stream中的背压（Backpressure）

backpressure是数据处理期间一般会出现问题。 主要意思就是数据传输期间，缓冲区会产生一定量的数据累积。如果数据的接收端具有复杂的操作，或者由于某种原因而变慢时，来自输入端的数据会逐渐累积，产生一些问题，例如阻塞。

要解决这个问题，必须有一个委托系统来确保数据从一个源到另一个源平滑地传输。不同的社区针对这个问题已经有了他们各自的解决方案。Unix管道和TCP套接字就是这方面的范例，并且这通常被称为流控制。在Node.js中，流被采用作为这个解决方案。

本指南的目的是进一步详细说明背压是什么，以及更详细地说明在Node.js的源代码中如何用流解决这个问题。本指南的第二部分将介绍一些最佳实践，以确保在实现流时，应用程序的代码是比较安全的和高性能的。

假设你对在Node.js中的backpressure，Buffer以及EventEmitters已经有一点点的熟悉。并且有一些关于Stream的经验。如果您还没有阅读这些文档，那么首先查看一下API文档是一个比较好的主意，因为它有助于在阅读本指南时扩展您的理解。

### 数据处理的问题

在计算机系统中，数据通过管道（pipes），套接字（sockets）和信号从一个进程传输到另一个进程。在Node.js中，有一种类似的机制 Stream。Stream 是很棒的！它为Node.js做了很多事情，几乎内部代码库的每个部分都使用该模块。作为开发人员，我们鼓励您使用它们！

``` javascript
const readline = require('readline');

// process.stdin and process.stdout are both instances of Streams
const rl = readline.createInterface({
	input: process.stdin,
	output: process.stdout
});

rl.question('Why should you use streams? ', (answer) => {
	console.log(`Maybe it's ${answer}, maybe it's because they are awesome! :)`);

	rl.close();
});

```

通过比较Node.js Stream实现的内部系统工具，可以证明通过流实现backpressure机制是一个很好的优化示例。

在一种情况下，我们将使用一个大文件（大约〜9gb）并使用熟悉的[zip(1)](https://linux.die.net/man/1/zip)工具对其进行压缩。

``` shell
$ zip The.Matrix.1080p.mkv
```

虽然这需要几分钟才能完成，但在另一个shell中，我们可能会运行一个带有Node.js模块的脚本，该模块zlib包含另一个压缩工具gzip(1)。

``` javascript
const gzip = require('zlib').createGzip();
const fs = require('fs');

const inp = fs.createReadStream('The.Matrix.1080p.mkv');
const out = fs.createWriteStream('The.Matrix.1080p.mkv.gz');

inp.pipe(gzip).pipe(out);
```

要测试结果，请尝试打开每个压缩文件。通过`zip(1)`工具压缩的文件在解压缩时会发现文件已损坏，而通过Stream完成的压缩，在解压缩时不会出错。

注意：在上面的示例中，我们使用`.pipe()`从一端到另一端获取数据。但是，请注意没有附加正确的错误处理程序。如果无法正确接收大量数据，则不会销毁Readable源或 gzip流。pump是一个实用工具，如果其中一个流失败或关闭，将销毁管道中的所有流，在这种情况下这种处理是必须的！

pump只有Nodejs 8.x或更早版本才需要，因为在Node10.x或更高版本中，pipeline被引入，用于替换pump。这是一种模块方法，用于管理流转发错误和正确清理流并在管道完成时提供回调。

以下是使用管道的示例：

``` javascript
const { pipeline } = require('stream');
const fs = require('fs');
const zlib = require('zlib');

// Use the pipeline API to easily pipe a series of streams
// together and get notified when the pipeline is fully done.
// A pipeline to gzip a potentially huge video file efficiently:

pipeline(
  fs.createReadStream('The.Matrix.1080p.mkv'),
  zlib.createGzip(),
  fs.createWriteStream('The.Matrix.1080p.mkv.gz'),
  (err) => {
    if (err) {
      console.error('Pipeline failed', err);
    } else {
      console.log('Pipeline succeeded');
    }
  }
);

```

您还可以调用promisify管道将其与async/await一起使用：

``` javascript
const stream = require('stream');
const fs = require('fs');
const zlib = require('zlib');

const pipeline = util.promisify(stream.pipeline);

async function run() {
    try {
        await pipeline(
            fs.createReadStream('The.Matrix.1080p.mkv'),
            zlib.createGzip(),
            fs.createWriteStream('The.Matrix.1080p.mkv.gz'),
        );
        console.log('Pipeline succeeded');
    } catch (err) {
        console.error('Pipeline failed', err);
    }
}
```

### 太多的数据，太快了

有些情况下，Readable流可能会比Writable流更快地提供数据 ———— 远远超过使用者处理数据的速度！

当发生这种情况时，所有数据块将会排队等待数据消费者处理。此时写入队列将变得越来越长，在整个过程完成之前，这些数据必须保存在内存中。

写入磁盘比从磁盘读取慢得多，因此，当我们尝试压缩文件并将其写入硬盘时，会发生backpressure，因为写入磁盘的速度无法跟上读取速度。请阅读：

``` shell
// Secretly the stream is saying: "whoa, whoa! hang on, this is way too much!"
// Data will begin to build up on the read-side of the data buffer as
// `write` tries to keep up with the incoming data flow.
inp.pipe(gzip).pipe(outputFile);
```

这就是backpressure机制很重要的原因。如果没有backpressure系统，该过程将耗尽系统的内存，这会显著影响其他进程，并独占系统的大部分资源直到完成。

这导致了一些事情：

- 减慢其他所有当前流程
- 垃圾收集器过载
- 内存资源耗尽
- 
在下面的例子中，我们将取出返回值的的 `.write()`功能，将其更改为true，这样可有效禁用在Node.js的核心backpressure支持。在对'modified'二进制文件的任何引用中，我们讨论的是在node没有`return ret;`行的情况下运行二进制文件，而是使用替换的`return true;`。

### GC(Garbage Collection)上的额外压力

我们来浏览下基准测试。使用上面的相同示例，我们进行了一些时间试验，以获得两个二进制文件处理的平均时间。